#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶å¤¹æ‰¹é‡AIè¯—æ­Œæ ‡ç­¾å¤„ç†å™¨
æ”¯æŒæŒ‰æ–‡ä»¶å¤¹æ‰¹é‡å¤„ç†å¤šä¸ªJSONæ–‡ä»¶ï¼Œç”Ÿæˆæ™ºèƒ½æ ‡ç­¾
æ”¯æŒæš‚åœå’Œç»­ä¼ åŠŸèƒ½
"""

import os
import json
import time
import argparse
import logging
import signal
import sys
from typing import List, Dict, Any
from deepseek_poem_analyzer import AIPoemAnalyzer
from progress_manager import ProgressManager, check_resume_processing, cleanup_progress_file
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('folder_ai_poem_processing.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FolderBatchPoemProcessor:
    """æ–‡ä»¶å¤¹æ‰¹é‡è¯—æ­Œå¤„ç†å™¨"""
    
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–æ–‡ä»¶å¤¹æ‰¹é‡å¤„ç†å™¨
        
        Args:
            api_key: DeepSeek APIå¯†é’¥
        """
        self.analyzer = AIPoemAnalyzer(api_key)
        self.progress_manager = ProgressManager()
        self.should_pause = False
        
        # è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼Œæ”¯æŒCtrl+Cæš‚åœ
        signal.signal(signal.SIGINT, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨ï¼Œæ”¯æŒCtrl+Cæš‚åœ"""
        print("\n\nâ¸ï¸  æ”¶åˆ°æš‚åœä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
        self.should_pause = True
        
    def scan_json_files(self, folder_path: str = "json") -> List[str]:
        """
        æ‰«ææ–‡ä»¶å¤¹ä¸­çš„JSONæ–‡ä»¶
        
        Args:
            folder_path: æ–‡ä»¶å¤¹è·¯å¾„
            
        Returns:
            JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        if not os.path.exists(folder_path):
            logger.error(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            raise FileNotFoundError(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
            
        json_files = []
        for filename in os.listdir(folder_path):
            if filename.endswith('.json'):
                json_files.append(os.path.join(folder_path, filename))
        
        json_files.sort(key=lambda x: int(os.path.basename(x).split('.')[0]))
        logger.info(f"æ‰«æåˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
        return json_files
    
    def load_poems_from_file(self, file_path: str) -> List[Dict]:
        """
        ä»å•ä¸ªJSONæ–‡ä»¶åŠ è½½è¯—æ­Œæ•°æ®
        
        Args:
            file_path: JSONæ–‡ä»¶è·¯å¾„
            
        Returns:
            è¯—æ­Œæ•°æ®åˆ—è¡¨
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                poems_data = json.load(f)
            
            # ä¸ºæ¯é¦–è¯—æ­Œæ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
            for poem in poems_data:
                poem['source_file'] = os.path.basename(file_path)
                
            logger.info(f"ä» {file_path} åŠ è½½äº† {len(poems_data)} é¦–è¯—æ­Œ")
            return poems_data
        except FileNotFoundError:
            logger.error(f"æœªæ‰¾åˆ°æ–‡ä»¶: {file_path}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"JSONæ–‡ä»¶è§£æå¤±è´¥ {file_path}: {e}")
            raise
    
    def process_folder(self, folder_path: str = "json",
                      start_file: int = 1,
                      end_file: int = None,
                      batch_size: int = 20,
                      delay: float = 1.0,
                      output_folder: str = "website_data",
                      resume: bool = False) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶ï¼Œæ”¯æŒæš‚åœå’Œç»­ä¼ 
        
        Args:
            folder_path: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
            start_file: å¼€å§‹æ–‡ä»¶ç¼–å·
            end_file: ç»“æŸæ–‡ä»¶ç¼–å·
            batch_size: æ‰¹æ¬¡å¤§å°
            delay: è¯·æ±‚é—´éš”
            output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
            resume: æ˜¯å¦æ¢å¤ä¹‹å‰çš„å¤„ç†
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        # æ‰«æJSONæ–‡ä»¶
        json_files = self.scan_json_files(folder_path)
        
        if end_file is None:
            end_file = len(json_files)
        
        # ç­›é€‰æ–‡ä»¶èŒƒå›´
        files_to_process = json_files[start_file-1:end_file]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¢å¤å¤„ç†
        if resume and check_resume_processing():
            print("ğŸ”„ æ£€æµ‹åˆ°æœªå®Œæˆçš„å¤„ç†ä»»åŠ¡ï¼Œæ­£åœ¨æ¢å¤...")
            remaining_files = self.progress_manager.get_remaining_files(files_to_process)
            files_to_process = remaining_files
            self.progress_manager.resume_processing()
        else:
            # å¼€å§‹æ–°çš„å¤„ç†
            self.progress_manager.start_processing(len(files_to_process))
        
        logger.info(f"å¤„ç†æ–‡ä»¶èŒƒå›´: {start_file} åˆ° {end_file}ï¼Œå…± {len(files_to_process)} ä¸ªæ–‡ä»¶")
        
        all_processed_poems = []
        file_stats = {}
        
        for file_path in files_to_process:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æš‚åœ
            if self.should_pause:
                print("\nâ¸ï¸ æ­£åœ¨æš‚åœå¤„ç†...")
                self.progress_manager.pause_processing()
                self.progress_manager.print_progress_summary()
                print("ğŸ’¡ æç¤º: ä½¿ç”¨ --resume å‚æ•°å¯ä»¥æ¢å¤å¤„ç†")
                return {"status": "paused", "processed_files": len(all_processed_poems)}
            
            file_name = os.path.basename(file_path)
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_name}")
            
            try:
                # è®¾ç½®å½“å‰å¤„ç†æ–‡ä»¶
                poems_data = self.load_poems_from_file(file_path)
                self.progress_manager.set_current_file(file_path, len(poems_data))
                
                # å¤„ç†è¯—æ­Œ
                processed_poems = self.analyzer.batch_analyze(
                    poems_data,
                    batch_size=batch_size,
                    delay=delay
                )
                
                # æ›´æ–°è¿›åº¦
                successful_count = len([p for p in processed_poems if 'ai_tags' in p])
                self.progress_manager.update_file_progress(len(processed_poems), successful_count)
                
                # ä¿å­˜å•ä¸ªæ–‡ä»¶çš„ç»“æœ
                output_file = os.path.join(output_folder, f"ai_enhanced_{file_name}")
                self.save_results(processed_poems, output_file)
                
                # ç»Ÿè®¡ä¿¡æ¯
                file_stats[file_name] = {
                    'total_poems': len(poems_data),
                    'successful_analysis': successful_count,
                    'failed_analysis': len([p for p in processed_poems if 'ai_tags' not in p])
                }
                
                # å®Œæˆæ–‡ä»¶å¤„ç†
                self.progress_manager.complete_file(file_path, successful_count, len(poems_data))
                
                all_processed_poems.extend(processed_poems)
                logger.info(f"æ–‡ä»¶ {file_name} å¤„ç†å®Œæˆ")
                
                # æ‰“å°è¿›åº¦
                self.progress_manager.print_progress_summary()
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {file_name} å¤±è´¥: {e}")
                self.progress_manager.mark_file_failed(file_path, str(e))
                file_stats[file_name] = {
                    'total_poems': 0,
                    'successful_analysis': 0,
                    'failed_analysis': 0,
                    'error': str(e)
                }
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        if all_processed_poems:
            merged_output = os.path.join(output_folder, "ai_enhanced_poems_merged.json")
            self.save_results(all_processed_poems, merged_output)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = self.generate_comprehensive_statistics(all_processed_poems)
        stats['file_statistics'] = file_stats
        
        # æ ‡è®°å¤„ç†å®Œæˆ
        self.progress_manager.complete_processing()
        
        return stats
    
    def save_results(self, processed_poems: List[Dict], output_file: str):
        """
        ä¿å­˜å¤„ç†ç»“æœ
        
        Args:
            processed_poems: å¤„ç†åçš„è¯—æ­Œæ•°æ®
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(processed_poems, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def generate_comprehensive_statistics(self, processed_poems: List[Dict]) -> Dict[str, Any]:
        """
        ç”Ÿæˆç»¼åˆç»Ÿè®¡ä¿¡æ¯
        
        Args:
            processed_poems: å¤„ç†åçš„è¯—æ­Œæ•°æ®
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        from collections import Counter
        
        stats = self.analyzer.generate_analysis_statistics(processed_poems)
        
        # æ·»åŠ æ›´å¤šç»Ÿè®¡ç»´åº¦
        stats['author_distribution'] = Counter()
        stats['dynasty_distribution'] = Counter()
        stats['file_distribution'] = Counter()
        stats['tag_coverage'] = {
            'has_styles': 0,
            'has_scenes': 0, 
            'has_emotions': 0,
            'has_themes': 0,
            'has_rhetoric': 0
        }
        
        for poem in processed_poems:
            # ä½œè€…åˆ†å¸ƒ
            stats['author_distribution'][poem.get('author', 'æœªçŸ¥')] += 1
            
            # æœä»£åˆ†å¸ƒ
            stats['dynasty_distribution'][poem.get('dynasty', 'æœªçŸ¥')] += 1
            
            # æ–‡ä»¶åˆ†å¸ƒ
            stats['file_distribution'][poem.get('source_file', 'æœªçŸ¥')] += 1
            
            # æ ‡ç­¾è¦†ç›–ç»Ÿè®¡
            if 'ai_tags' in poem:
                tags = poem['ai_tags']
                if tags.get('styles'):
                    stats['tag_coverage']['has_styles'] += 1
                if tags.get('scenes'):
                    stats['tag_coverage']['has_scenes'] += 1
                if tags.get('emotions'):
                    stats['tag_coverage']['has_emotions'] += 1
                if tags.get('themes'):
                    stats['tag_coverage']['has_themes'] += 1
                if tags.get('rhetoric'):
                    stats['tag_coverage']['has_rhetoric'] += 1
        
        return stats
    
    def save_statistics(self, stats: Dict[str, Any], 
                       stats_file: str = "website_data/folder_ai_analysis_statistics.json"):
        """
        ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯
            stats_file: ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
        """
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
    
    def print_statistics_summary(self, stats: Dict[str, Any]):
        """
        æ‰“å°ç»Ÿè®¡æ‘˜è¦
        
        Args:
            stats: ç»Ÿè®¡ä¿¡æ¯
        """
        print("\n" + "="*60)
        print("ğŸ“Š æ–‡ä»¶å¤¹æ‰¹é‡AIè¯—æ­Œåˆ†æç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        
        print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
        print(f"  åˆ†æè¯—æ­Œæ€»æ•°: {stats['total_analyzed']}")
        print(f"  æˆåŠŸåˆ†ææ•°é‡: {stats['successful_analysis']}")
        print(f"  åˆ†ææˆåŠŸç‡: {stats['successful_analysis']/stats['total_analyzed']*100:.1f}%")
        
        print(f"\nğŸ“ æ–‡ä»¶åˆ†å¸ƒ (Top 10):")
        for file_name, count in stats['file_distribution'].most_common(10):
            print(f"  {file_name}: {count}é¦–")
            
        print(f"\nğŸ­ é£æ ¼åˆ†å¸ƒ (Top 5):")
        for style, count in stats['style_distribution'].most_common(5):
            print(f"  {style}: {count}é¦–")
            
        print(f"\nğŸŒ„ åœºæ™¯åˆ†å¸ƒ (Top 5):")
        for scene, count in stats['scene_distribution'].most_common(5):
            print(f"  {scene}: {count}é¦–")
            
        print(f"\nğŸ’– æƒ…æ„Ÿåˆ†å¸ƒ (Top 5):")
        for emotion, count in stats['emotion_distribution'].most_common(5):
            print(f"  {emotion}: {count}é¦–")
            
        print(f"\nğŸ“š ä¸»é¢˜åˆ†å¸ƒ (Top 5):")
        for theme, count in stats['theme_distribution'].most_common(5):
            print(f"  {theme}: {count}é¦–")
            
        print(f"\nğŸ”‘ çƒ­é—¨å…³é”®è¯ (Top 10):")
        for keyword, count in stats['top_keywords'].most_common(10):
            print(f"  {keyword}: {count}æ¬¡")
            
        print(f"\nğŸ‘¤ çƒ­é—¨ä½œè€… (Top 5):")
        for author, count in stats['author_distribution'].most_common(5):
            print(f"  {author}: {count}é¦–")
            
        print(f"\nğŸ›ï¸ æœä»£åˆ†å¸ƒ:")
        for dynasty, count in stats['dynasty_distribution'].most_common():
            print(f"  {dynasty}: {count}é¦–")
            
        print(f"\nğŸ·ï¸ æ ‡ç­¾è¦†ç›–æƒ…å†µ:")
        coverage = stats['tag_coverage']
        total = stats['total_analyzed']
        print(f"  é£æ ¼æ ‡ç­¾: {coverage['has_styles']}/{total} ({coverage['has_styles']/total*100:.1f}%)")
        print(f"  åœºæ™¯æ ‡ç­¾: {coverage['has_scenes']}/{total} ({coverage['has_scenes']/total*100:.1f}%)")
        print(f"  æƒ…æ„Ÿæ ‡ç­¾: {coverage['has_emotions']}/{total} ({coverage['has_emotions']/total*100:.1f}%)")
        print(f"  ä¸»é¢˜æ ‡ç­¾: {coverage['has_themes']}/{total} ({coverage['has_themes']/total*100:.1f}%)")
        print(f"  ä¿®è¾æ ‡ç­¾: {coverage['has_rhetoric']}/{total} ({coverage['has_rhetoric']/total*100:.1f}%)")
        
        # æ–‡ä»¶ç»Ÿè®¡è¯¦æƒ…
        if 'file_statistics' in stats:
            print(f"\nğŸ“‹ æ–‡ä»¶å¤„ç†è¯¦æƒ…:")
            for file_name, file_stat in stats['file_statistics'].items():
                if 'error' in file_stat:
                    print(f"  {file_name}: âŒ å¤„ç†å¤±è´¥ - {file_stat['error']}")
                else:
                    success_rate = file_stat['successful_analysis'] / file_stat['total_poems'] * 100 if file_stat['total_poems'] > 0 else 0
                    print(f"  {file_name}: {file_stat['successful_analysis']}/{file_stat['total_poems']} ({success_rate:.1f}%)")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ–‡ä»¶å¤¹æ‰¹é‡AIè¯—æ­Œæ ‡ç­¾å¤„ç†å™¨')
    parser.add_argument('--api-key', help='DeepSeek APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰')
    parser.add_argument('--folder', default='json', help='è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--output-folder', default='website_data', help='è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„')
    parser.add_argument('--start-file', type=int, default=1, help='å¼€å§‹æ–‡ä»¶ç¼–å·')
    parser.add_argument('--end-file', type=int, help='ç»“æŸæ–‡ä»¶ç¼–å·')
    parser.add_argument('--batch-size', type=int, default=20, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--delay', type=float, default=1.0, help='è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--sample-files', type=int, help='æ ·æœ¬æ–‡ä»¶æ•°é‡ï¼ˆæµ‹è¯•ç”¨ï¼‰')
    parser.add_argument('--resume', action='store_true', help='æ¢å¤ä¹‹å‰çš„å¤„ç†')
    parser.add_argument('--show-progress', action='store_true', help='æ˜¾ç¤ºå½“å‰è¿›åº¦')
    parser.add_argument('--cleanup', action='store_true', help='æ¸…ç†è¿›åº¦æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ¸…ç†è¿›åº¦æ–‡ä»¶
    if args.cleanup:
        cleanup_progress_file()
        print("âœ… è¿›åº¦æ–‡ä»¶å·²æ¸…ç†")
        return
    
    # æ˜¾ç¤ºè¿›åº¦
    if args.show_progress:
        progress_manager = ProgressManager()
        progress_manager.print_progress_summary()
        return
    
    # è·å–APIå¯†é’¥
    api_key = args.api_key or os.getenv('DEEPSEEK_API_KEY')
    if not api_key:
        print("è¯·æä¾›DeepSeek APIå¯†é’¥")
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  1. åœ¨.envæ–‡ä»¶ä¸­è®¾ç½® DEEPSEEK_API_KEY=your_key")
        print("  2. è®¾ç½®ç³»ç»Ÿç¯å¢ƒå˜é‡: set DEEPSEEK_API_KEY=your_key")
        print("  3. æˆ–ä½¿ç”¨å‚æ•°: --api-key your_key")
        return
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = FolderBatchPoemProcessor(api_key)
        
        # å¤„ç†æ–‡ä»¶å¤¹
        stats = processor.process_folder(
            folder_path=args.folder,
            start_file=args.start_file,
            end_file=args.end_file,
            batch_size=args.batch_size,
            delay=args.delay,
            output_folder=args.output_folder,
            resume=args.resume
        )
        
        # æ£€æŸ¥æ˜¯å¦æš‚åœ
        if stats.get('status') == 'paused':
            print("\nâ¸ï¸ å¤„ç†å·²æš‚åœ")
            print("ğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¢å¤å¤„ç†:")
            print(f"   python folder_batch_poem_processor.py --resume")
            return
        
        # ä¿å­˜ç»Ÿè®¡
        processor.save_statistics(stats)
        
        # æ‰“å°æ‘˜è¦
        processor.print_statistics_summary(stats)
        
        print(f"\nğŸ‰ æ–‡ä»¶å¤¹æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ å¢å¼ºæ•°æ®: {args.output_folder}/ai_enhanced_*.json")
        print(f"ğŸ“ åˆå¹¶æ•°æ®: {args.output_folder}/ai_enhanced_poems_merged.json")
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {args.output_folder}/folder_ai_analysis_statistics.json")
        print(f"ğŸ“ å¤„ç†æ—¥å¿—: folder_ai_poem_processing.log")
        
    except Exception as e:
        logger.error(f"å¤„ç†å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()